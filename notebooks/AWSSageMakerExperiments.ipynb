{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training and Hosting your Algorithm in Amazon SageMaker\n",
    "\n",
    "Once you have your container packaged, you can use it to train and serve models. Let's do that with the algorithm we made above.\n",
    "\n",
    "## Set up the environment\n",
    "\n",
    "Here we specify a bucket to use and the role that will be used for working with SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'bgc-data'\n",
    "PREFIX = 'sagemaker/lstm-example/'\n",
    "DATA_PREFIX = os.path.join(PREFIX, 'data')\n",
    "SAMPLES_PREFIX = os.path.join(DATA_PREFIX, 'samples')\n",
    "VALIDATION_PREFIX = os.path.join(DATA_PREFIX, 'validation')\n",
    "FILES_PREFIX = os.path.join(DATA_PREFIX, 'files')\n",
    "MODEL_PREFIX = os.path.join(PREFIX, 'model')\n",
    "ROLE = 'arn:aws:iam::487322236248:role/bgc-sagemaker-role'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session\n",
    "\n",
    "The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = None\n",
    "s3 = None\n",
    "\n",
    "def reload_aws_session():\n",
    "    global s3, sess\n",
    "    sess = sage.Session()\n",
    "    s3 = sess.boto_session.resource('s3')\n",
    "\n",
    "reload_aws_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, we're using some the classic [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which we have included. \n",
    "\n",
    "We can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.upload_data('../data/training/positive/CF_bgcs.csv', key_prefix=SAMPLES_PREFIX, bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess.upload_data('../data/training/negative/geneswap_negatives.csv', key_prefix=SAMPLES_PREFIX, bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.upload_data('../data/features/pfam2vec-experiments/pfam2vec_top.bin', key_prefix=FILES_PREFIX, bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.upload_data('../data/evaluation/labelled-bootstrap/splits', key_prefix=VALIDATION_PREFIX, bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PFAM2VEC_PATH = os.path.join(FILES_PREFIX, '/pfam2vec')\n",
    "sess.upload_data('../data/features/pfam2vec-experiments/iterations/', key_prefix=PFAM2VEC_PATH, bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def s3_copy(sourcekey, targetkey, files=None):\n",
    "    print('Copying \"{}\" to \"{}\"'.format(sourcekey, targetkey))\n",
    "    if not files:\n",
    "        copy_source = {\n",
    "            'Bucket': BUCKET_NAME,\n",
    "            'Key': sourcekey\n",
    "        }\n",
    "        s3.meta.client.copy(copy_source, BUCKET_NAME, targetkey)\n",
    "        return\n",
    "    for file in files:\n",
    "        copy_source = {\n",
    "            'Bucket': BUCKET_NAME,\n",
    "            'Key': os.path.join(sourcekey, file)\n",
    "        }\n",
    "        print(' Copying \"{}\"'.format(file))\n",
    "        s3.meta.client.copy(copy_source, BUCKET_NAME, os.path.join(targetkey, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use SageMaker to fit our algorithm, we'll create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n",
    "\n",
    "* The __container name__. This is constructed as in the shell commands above.\n",
    "* The __role__. As defined above.\n",
    "* The __instance count__ which is the number of machines to use for training.\n",
    "* The __instance type__ which is the type of machine to use for training.\n",
    "* The __output path__ determines where the model artifact will be written.\n",
    "* The __session__ is the SageMaker session object that we defined above.\n",
    "\n",
    "Then we use fit() on the estimator to train against the data that we uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model_name, model_config, data_path, model_path, mode='cpu', wait=False):\n",
    "    if mode == 'cpu':\n",
    "        # Compute optimized 16 CPUs $1.114/hour: ml.c4.4xlarge\n",
    "        # Compute optimized 36 CPUs $2.227/hour: ml.c4.8xlarge\n",
    "        instance_type = 'ml.m5.large'#'ml.c4.4xlarge'\n",
    "        image_name='bgc-model-cpu'\n",
    "    elif mode == 'gpu':\n",
    "        # GPU 1xV100 $4.284/hour: ml.p3.2xlarge\n",
    "        instance_type = 'ml.p3.2xlarge'\n",
    "        image_name='bgc-model-gpu'\n",
    "    else:\n",
    "        raise ValueError('Invalid mode')\n",
    "    account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "    region = sess.boto_session.region_name\n",
    "    image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, image_name)\n",
    "    print('Image:', image)\n",
    "    input_path = \"s3://{}/{}\".format(BUCKET_NAME, data_path)\n",
    "    output_path = \"s3://{}/{}\".format(BUCKET_NAME, model_path)\n",
    "    \n",
    "    hyperparameters = {k: json.dumps(v) for k, v in model_config.items()}\n",
    "    model = sage.estimator.Estimator(image,\n",
    "                           ROLE, 1, instance_type, \n",
    "                           output_path=output_path,\n",
    "                           sagemaker_session=sess, \n",
    "                           hyperparameters=hyperparameters)\n",
    "\n",
    "    print('Fitting on data folder: {}'.format(input_path))\n",
    "    job_name = '{}-{}'.format(model_name, datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "    model.fit(input_path, wait=wait, job_name=job_name)\n",
    "\n",
    "    return job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_and_run_job(model_name, model_config, training_files, validation_files, pfam2vec_file, wait=False):\n",
    "    job_prefix = os.path.join('sagemaker', 'bootstrap')\n",
    "    job_data_path = os.path.join(job_prefix, 'data', model_name)\n",
    "    job_model_path = os.path.join(job_prefix, 'models')\n",
    "\n",
    "    objects_to_delete = s3.meta.client.list_objects(Bucket=BUCKET_NAME, Prefix=job_data_path)\n",
    "    delete_keys = [{'Key' : k} for k in [obj['Key'] for obj in objects_to_delete.get('Contents', [])]]\n",
    "    \n",
    "    print('Deleting {} existing files: {}'.format(len(delete_keys), delete_keys))\n",
    "    if delete_keys:\n",
    "        s3.meta.client.delete_objects(Bucket=\"MyBucket\", Delete={'Objects': delete_keys})\n",
    "\n",
    "    job_samples_path = os.path.join(job_data_path, 'samples')\n",
    "    s3_copy(SAMPLES_PREFIX, job_samples_path, files=training_files)\n",
    "    \n",
    "    if validation_files:\n",
    "        job_validation_path = os.path.join(job_data_path, 'validation')\n",
    "        s3_copy(VALIDATION_PREFIX, job_validation_path, files=validation_files)\n",
    "\n",
    "    job_pfam2vec_path = os.path.join(job_data_path, 'files', 'pfam2vec.bin')\n",
    "    pfam2vec_source = os.path.join(FILES_PREFIX, pfam2vec_file)\n",
    "    s3_copy(pfam2vec_source, job_pfam2vec_path)\n",
    "\n",
    "    job_name = run_training(model_name, model_config, data_path=job_data_path, model_path=job_model_path, mode='cpu', wait=wait)\n",
    "    model_path = os.path.join(job_model_path, job_name)\n",
    "    \n",
    "    return {\n",
    "        'submitted': datetime.now(),\n",
    "        'model_path': model_path,\n",
    "        'job_name': job_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 4 existing files: [{'Key': 'sagemaker/bootstrap/data/lstm-test/files/pfam2vec.bin'}, {'Key': 'sagemaker/bootstrap/data/lstm-test/samples/CF_bgcs.csv'}, {'Key': 'sagemaker/bootstrap/data/lstm-test/samples/geneswap_negatives.csv'}, {'Key': 'sagemaker/bootstrap/data/lstm-test/validation/split_0_train.csv'}]\n",
      "Copying \"sagemaker/lstm-example/data/samples\" to \"sagemaker/bootstrap/data/lstm-test/samples\"\n",
      " Copying \"CF_bgcs.csv\"\n",
      " Copying \"geneswap_negatives.csv\"\n",
      "Copying \"sagemaker/lstm-example/data/validation\" to \"sagemaker/bootstrap/data/lstm-test/validation\"\n",
      " Copying \"split_0_train.csv\"\n",
      "Copying \"sagemaker/lstm-example/data/files/pfam2vec/pfam2vec_corpus-1e-02_skipgram_100dim_5win_8iter.bin\" to \"sagemaker/bootstrap/data/lstm-test/files/pfam2vec.bin\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: lstm-test-2018-08-16-01-36-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 487322236248.dkr.ecr.us-east-1.amazonaws.com/bgc-model-cpu:latest\n",
      "Fitting on data folder: s3://bgc-data/sagemaker/bootstrap/data/lstm-test\n",
      ".............................................\n",
      "\u001b[31mRunning with python sys.version_info(major=3, minor=6, micro=5, releaselevel='final', serial=0)\u001b[0m\n",
      "\u001b[31mStarting the training.\u001b[0m\n",
      "\u001b[31mLoaded config:\u001b[0m\n",
      "\u001b[31m{'build_params': {'batch_size': 64, 'hidden_size': 128, 'stateful': True}, 'fit_params': {'timesteps': 256, 'validation_size': 0, 'num_epochs': 1, 'gpus': 0, 'verbose': 1, 'learning_rate': 0.0001, 'positive_weight': 1}, 'type': 'KerasRNN', 'input_params': {'features': [{'type': 'ProteinBorderTransformer'}, {'type': 'Pfam2VecTransformer', 'vector_path': '/opt/ml/input/data/training/files/pfam2vec.bin'}]}}\u001b[0m\n",
      "\u001b[31mLoaded model:\u001b[0m\n",
      "\u001b[31m{'build_params': {'batch_size': 64, 'hidden_size': 128, 'stateful': True},\n",
      " 'fit_params': {'gpus': 0,\n",
      "                'learning_rate': 0.0001,\n",
      "                'num_epochs': 1,\n",
      "                'positive_weight': 1,\n",
      "                'timesteps': 256,\n",
      "                'validation_size': 0,\n",
      "                'verbose': 1},\n",
      " 'input_params': {'features': [{'type': 'ProteinBorderTransformer'},\n",
      "                               {'type': 'Pfam2VecTransformer',\n",
      "                                'vector_path': '/opt/ml/input/data/training/files/pfam2vec.bin'}]},\n",
      " 'type': 'KerasRNN'}\u001b[0m\n",
      "\u001b[31mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[31mReading train samples:\u001b[0m\n",
      "\u001b[31mLoaded 617 samples and 35352 domains from /opt/ml/input/data/training/samples/CF_bgcs.csv\u001b[0m\n",
      "\u001b[31mLoaded 10128 samples and 702158 domains from /opt/ml/input/data/training/samples/geneswap_negatives.csv\u001b[0m\n",
      "\u001b[31mReading validation samples:\u001b[0m\n",
      "\u001b[31mLoaded 4 samples and 33865 domains from /opt/ml/input/data/training/validation/split_0_train.csv\u001b[0m\n",
      "\u001b[31mProgress will be saved to: /opt/ml/model/log/\u001b[0m\n",
      "\u001b[31mUsing optimizer adam {'lr': 0.0001}\u001b[0m\n",
      "\u001b[31mUsing optimizer adam {'lr': 0.0001}\u001b[0m\n",
      "\u001b[31mWarning: Not using positive_weight \"1\" on external validation set!\u001b[0m\n",
      "\u001b[31mValidating on external validation set of 4 samples\u001b[0m\n",
      "\u001b[31mFilling to batch size shape (64, 19601, 102) (127M values)...\u001b[0m\n",
      "\u001b[31mFilling done.\u001b[0m\n",
      "\u001b[31mInitializing generator of 46 batches from sequence length 737510\u001b[0m\n",
      "\u001b[31m2018-08-15 23:40:27.438619: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[31mEpoch 1/1\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error training lstm-test-2018-08-16-01-36-04: Failed Reason: ClientError: Please use an instance type with more memory, or reduce the size of training data processed on an instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-966ccd714e29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mvalidation_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpfam2vec_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpfam2vec_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[0mtest_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-2522af43b521>\u001b[0m in \u001b[0;36mcopy_files_and_run_job\u001b[0;34m(model_name, model_config, training_files, validation_files, pfam2vec_file, wait)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0ms3_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpfam2vec_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_pfam2vec_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mjob_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-205-c1df2912cc49>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model_name, model_config, data_path, model_path, mode, wait)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting on data folder: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mjob_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}-{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d-%H-%M-%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Completed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FailureReason'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(No reason provided)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error training {}: {} Reason: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error training lstm-test-2018-08-16-01-36-04: Failed Reason: ClientError: Please use an instance type with more memory, or reduce the size of training data processed on an instance."
     ]
    }
   ],
   "source": [
    "test_config = {\n",
    "  \"type\": \"KerasRNN\",\n",
    "  \"build_params\": {\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 128,\n",
    "    \"stateful\": True\n",
    "  },\n",
    "  \"fit_params\": {\n",
    "    \"timesteps\": 256,\n",
    "    \"validation_size\": 0,\n",
    "    \"num_epochs\": 1,\n",
    "    \"gpus\": 0,\n",
    "    \"verbose\": 1,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"positive_weight\": 1\n",
    "  },\n",
    "  \"input_params\": {\n",
    "    \"features\": [\n",
    "      {\n",
    "        \"type\": \"ProteinBorderTransformer\"\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"Pfam2VecTransformer\",\n",
    "        \"vector_path\": \"/opt/ml/input/data/training/files/pfam2vec.bin\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "pfam2vec_file=os.path.join('pfam2vec', 'pfam2vec_corpus-1e-02_skipgram_100dim_5win_8iter.bin')\n",
    "validation_files = ['split_0_train.csv']\n",
    "\n",
    "reload_aws_session()\n",
    "\n",
    "test_job = copy_files_and_run_job(\n",
    "    model_name='lstm-test', \n",
    "    model_config=test_config, \n",
    "    training_files=['CF_bgcs.csv', 'geneswap_negatives.csv'],\n",
    "    validation_files=validation_files, \n",
    "    pfam2vec_file=pfam2vec_file,\n",
    "    wait=True\n",
    ")\n",
    "test_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 240\n",
      "{'lstm': 128, 'pfamdim': 100, 'pfamiter': 8, 'posweight': 1, 'split': 0, 'training_files': ['CF_bgcs.csv', 'geneswap_negatives.csv']}\n",
      "{'lstm': 128, 'pfamdim': 100, 'pfamiter': 8, 'posweight': 1, 'split': 1, 'training_files': ['CF_bgcs.csv', 'geneswap_negatives.csv']}\n",
      "{'lstm': 128, 'pfamdim': 100, 'pfamiter': 8, 'posweight': 1, 'split': 2, 'training_files': ['CF_bgcs.csv', 'geneswap_negatives.csv']}\n",
      "{'lstm': 128, 'pfamdim': 100, 'pfamiter': 8, 'posweight': 1, 'split': 3, 'training_files': ['CF_bgcs.csv', 'geneswap_negatives.csv']}\n",
      "{'lstm': 128, 'pfamdim': 100, 'pfamiter': 8, 'posweight': 1, 'split': 4, 'training_files': ['CF_bgcs.csv', 'geneswap_negatives.csv']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_params = ParameterGrid({\n",
    "    'lstm' : [128, 256],\n",
    "    'pfamdim' : [50, 100, 200, 300],\n",
    "    'pfamiter' : [8, 32, 64],\n",
    "    'posweight' : [1, 16.415],\n",
    "    'split': [0, 1, 2, 3, 4],\n",
    "    'training_files': [ ['CF_bgcs.csv', 'geneswap_negatives.csv'] ]\n",
    "})\n",
    "print('Total', len(grid_params))\n",
    "grid_params = ParameterGrid({\n",
    "    'lstm' : [128],\n",
    "    'pfamdim' : [100],\n",
    "    'pfamiter' : [8],\n",
    "    'posweight' : [1],\n",
    "    'split': [0, 1, 2, 3, 4],\n",
    "    'training_files': [ ['CF_bgcs.csv', 'geneswap_negatives.csv'] ]\n",
    "})\n",
    "tasks = list(grid_params)\n",
    "for t in tasks[:5]:\n",
    "    print(t)\n",
    "len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_task_arg(k, v):\n",
    "    formatted = str(v)\n",
    "    if isinstance(v, list):\n",
    "        if v == ['CF_bgcs.csv', 'geneswap_negatives.csv']:\n",
    "            formatted = 'bgc-blastn'\n",
    "            k = ''\n",
    "        elif v == ['CF_bgcs.csv', 'geneswap_negatives.first_neg.csv']:\n",
    "            formatted = 'first_neg'\n",
    "            k = ''\n",
    "        else:\n",
    "            raise ValueError('No shortcut for value {} = {}'.format(k, v))\n",
    "        \n",
    "    return re.sub('[^a-zA-Z0-9-]+', '-', formatted+k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload_aws_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128lstm-100pfamdim-8pfamiter-1posweight-0split-bgc-blastn\n",
      "128lstm-100pfamdim-8pfamiter-1posweight-1split-bgc-blastn\n",
      "128lstm-100pfamdim-8pfamiter-1posweight-2split-bgc-blastn\n",
      "128lstm-100pfamdim-8pfamiter-1posweight-3split-bgc-blastn\n",
      "128lstm-100pfamdim-8pfamiter-1posweight-4split-bgc-blastn\n",
      "Submitted 2 jobs!\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    model_name = '-'.join(format_task_arg(k, task[k]) for k in sorted(list(task)))\n",
    "    print(model_name)\n",
    "    model_config = {\n",
    "      \"type\": \"KerasRNN\",\n",
    "      \"build_params\": {\n",
    "        \"batch_size\": 64,\n",
    "        \"hidden_size\": task['lstm'],\n",
    "        \"stateful\": True\n",
    "      },\n",
    "      \"fit_params\": {\n",
    "        \"timesteps\": 256,\n",
    "        \"validation_size\": 0,\n",
    "        \"num_epochs\": 250,\n",
    "        \"early_stop_monitor\": \"val_auc_roc\",\n",
    "        \"early_stop_min_delta\": 0.0005,\n",
    "        \"early_stop_patience\": 20,\n",
    "        \"early_stop_mode\": \"max\",\n",
    "        \"gpus\": 0,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"positive_weight\": task['posweight']\n",
    "      },\n",
    "      \"input_params\": {\n",
    "        \"features\": [\n",
    "          {\n",
    "            \"type\": \"ProteinBorderTransformer\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"Pfam2VecTransformer\",\n",
    "            \"vector_path\": \"/opt/ml/input/data/training/files/pfam2vec.bin\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    pfam2vec_file=os.path.join('pfam2vec', 'pfam2vec_corpus-1e-02_skipgram_{}dim_5win_{}iter.bin'.format(task['pfamdim'], task['pfamiter']))\n",
    "    validation_files = ['split_{}_train.csv'.format(task['split'])]\n",
    "    jobs[model_name] = copy_files_and_run_job(\n",
    "        model_name=model_name, \n",
    "        model_config=model_config, \n",
    "        training_files=training_files,\n",
    "        validation_files=validation_files, \n",
    "        pfam2vec_file=pfam2vec_file\n",
    "    )\n",
    "\n",
    "print('Submitted {} jobs!'.format(len(jobs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bgc-model-cpu-2018-08-15-16-05-01-246'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs['128lstm_100pfamdim_8pfamiter_1posweight']['job_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting on data folder: s3://bgc-data/sagemaker/job1/data\n",
      "Result will be saved to: s3://bgc-data/sagemaker/job1/model/ \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
